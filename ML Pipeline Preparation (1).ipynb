{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster.db')\n",
    "df = pd.read_sql_table('disaster',con=engine)\n",
    "# Names all columns starting at the fifth column as \"categories\"\n",
    "categories = df.columns[4:]\n",
    "# X contains values from message column and y contains values from categories columns\n",
    "X = df[['message']].values[:, 0]\n",
    "y = df[categories].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text) \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]))\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier())),\n",
    "        ])\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_d...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = model_pipeline()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('features', FeatureUnion(n_jobs=1,\n",
       "          transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "        steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]))],\n",
       "          transformer_weights=None)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))], 'features': FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]))],\n",
       "        transformer_weights=None), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1), 'features__n_jobs': 1, 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]))], 'features__transformer_weights': None, 'features__text_pipeline': Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]), 'features__text_pipeline__memory': None, 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f1510f8b598>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))], 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f1510f8b598>, vocabulary=None), 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'features__text_pipeline__vect__analyzer': 'word', 'features__text_pipeline__vect__binary': False, 'features__text_pipeline__vect__decode_error': 'strict', 'features__text_pipeline__vect__dtype': numpy.int64, 'features__text_pipeline__vect__encoding': 'utf-8', 'features__text_pipeline__vect__input': 'content', 'features__text_pipeline__vect__lowercase': True, 'features__text_pipeline__vect__max_df': 1.0, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__min_df': 1, 'features__text_pipeline__vect__ngram_range': (1,\n",
       "  1), 'features__text_pipeline__vect__preprocessor': None, 'features__text_pipeline__vect__stop_words': None, 'features__text_pipeline__vect__strip_accents': None, 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>, 'features__text_pipeline__vect__vocabulary': None, 'features__text_pipeline__tfidf__norm': 'l2', 'features__text_pipeline__tfidf__smooth_idf': True, 'features__text_pipeline__tfidf__sublinear_tf': False, 'features__text_pipeline__tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 10, 'clf__estimator__n_jobs': 1, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False), 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multioutput_classification_report(y_true, y_pred):\n",
    "    for i in range(0, len(categories)):\n",
    "        print(categories[i] + \":\")\n",
    "        print(\"\\tAccuracy: {:.4f}\\tPrecision: {:.4f}\\tRecall: {:.4f}\\tF1_score: {:.4f}\".format(\n",
    "            accuracy_score(y_true[:, i], y_pred[:, i]),\n",
    "            precision_score(y_true[:, i], y_pred[:, i], average='weighted'),\n",
    "            recall_score(y_true[:, i], y_pred[:, i], average='weighted'),\n",
    "            f1_score(y_true[:, i], y_pred[:, i], average='weighted')\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.9905\tPrecision: 0.9905\tRecall: 0.9905\tF1_score: 0.9905\n",
      "request:\n",
      "\tAccuracy: 0.9870\tPrecision: 0.9872\tRecall: 0.9870\tF1_score: 0.9869\n",
      "offer:\n",
      "\tAccuracy: 0.9988\tPrecision: 0.9988\tRecall: 0.9988\tF1_score: 0.9987\n",
      "aid_related:\n",
      "\tAccuracy: 0.9852\tPrecision: 0.9854\tRecall: 0.9852\tF1_score: 0.9852\n",
      "medical_help:\n",
      "\tAccuracy: 0.9886\tPrecision: 0.9887\tRecall: 0.9886\tF1_score: 0.9882\n",
      "medical_products:\n",
      "\tAccuracy: 0.9919\tPrecision: 0.9919\tRecall: 0.9919\tF1_score: 0.9915\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.9934\tPrecision: 0.9934\tRecall: 0.9934\tF1_score: 0.9930\n",
      "security:\n",
      "\tAccuracy: 0.9957\tPrecision: 0.9957\tRecall: 0.9957\tF1_score: 0.9955\n",
      "military:\n",
      "\tAccuracy: 0.9952\tPrecision: 0.9952\tRecall: 0.9952\tF1_score: 0.9950\n",
      "water:\n",
      "\tAccuracy: 0.9942\tPrecision: 0.9942\tRecall: 0.9942\tF1_score: 0.9941\n",
      "food:\n",
      "\tAccuracy: 0.9946\tPrecision: 0.9946\tRecall: 0.9946\tF1_score: 0.9946\n",
      "shelter:\n",
      "\tAccuracy: 0.9914\tPrecision: 0.9915\tRecall: 0.9914\tF1_score: 0.9912\n",
      "clothing:\n",
      "\tAccuracy: 0.9976\tPrecision: 0.9976\tRecall: 0.9976\tF1_score: 0.9975\n",
      "money:\n",
      "\tAccuracy: 0.9952\tPrecision: 0.9952\tRecall: 0.9952\tF1_score: 0.9949\n",
      "missing_people:\n",
      "\tAccuracy: 0.9974\tPrecision: 0.9974\tRecall: 0.9974\tF1_score: 0.9972\n",
      "refugees:\n",
      "\tAccuracy: 0.9933\tPrecision: 0.9933\tRecall: 0.9933\tF1_score: 0.9929\n",
      "death:\n",
      "\tAccuracy: 0.9942\tPrecision: 0.9942\tRecall: 0.9942\tF1_score: 0.9940\n",
      "other_aid:\n",
      "\tAccuracy: 0.9772\tPrecision: 0.9776\tRecall: 0.9772\tF1_score: 0.9763\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.9851\tPrecision: 0.9853\tRecall: 0.9851\tF1_score: 0.9842\n",
      "transport:\n",
      "\tAccuracy: 0.9908\tPrecision: 0.9909\tRecall: 0.9908\tF1_score: 0.9904\n",
      "buildings:\n",
      "\tAccuracy: 0.9932\tPrecision: 0.9932\tRecall: 0.9932\tF1_score: 0.9930\n",
      "electricity:\n",
      "\tAccuracy: 0.9963\tPrecision: 0.9963\tRecall: 0.9963\tF1_score: 0.9961\n",
      "tools:\n",
      "\tAccuracy: 0.9984\tPrecision: 0.9984\tRecall: 0.9984\tF1_score: 0.9983\n",
      "hospitals:\n",
      "\tAccuracy: 0.9969\tPrecision: 0.9969\tRecall: 0.9969\tF1_score: 0.9967\n",
      "shops:\n",
      "\tAccuracy: 0.9985\tPrecision: 0.9985\tRecall: 0.9985\tF1_score: 0.9983\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9974\tPrecision: 0.9974\tRecall: 0.9974\tF1_score: 0.9972\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.9900\tPrecision: 0.9900\tRecall: 0.9900\tF1_score: 0.9893\n",
      "weather_related:\n",
      "\tAccuracy: 0.9879\tPrecision: 0.9880\tRecall: 0.9879\tF1_score: 0.9878\n",
      "floods:\n",
      "\tAccuracy: 0.9913\tPrecision: 0.9914\tRecall: 0.9913\tF1_score: 0.9911\n",
      "storm:\n",
      "\tAccuracy: 0.9941\tPrecision: 0.9941\tRecall: 0.9941\tF1_score: 0.9940\n",
      "fire:\n",
      "\tAccuracy: 0.9980\tPrecision: 0.9980\tRecall: 0.9980\tF1_score: 0.9978\n",
      "earthquake:\n",
      "\tAccuracy: 0.9954\tPrecision: 0.9954\tRecall: 0.9954\tF1_score: 0.9954\n",
      "cold:\n",
      "\tAccuracy: 0.9980\tPrecision: 0.9980\tRecall: 0.9980\tF1_score: 0.9979\n",
      "other_weather:\n",
      "\tAccuracy: 0.9900\tPrecision: 0.9901\tRecall: 0.9900\tF1_score: 0.9895\n",
      "direct_report:\n",
      "\tAccuracy: 0.9817\tPrecision: 0.9819\tRecall: 0.9817\tF1_score: 0.9814\n"
     ]
    }
   ],
   "source": [
    "# Predicts messages in the training set based on categories in the training set\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "multioutput_classification_report(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.8071\tPrecision: 0.7945\tRecall: 0.8071\tF1_score: 0.7976\n",
      "request:\n",
      "\tAccuracy: 0.8772\tPrecision: 0.8690\tRecall: 0.8772\tF1_score: 0.8597\n",
      "offer:\n",
      "\tAccuracy: 0.9960\tPrecision: 0.9920\tRecall: 0.9960\tF1_score: 0.9940\n",
      "aid_related:\n",
      "\tAccuracy: 0.7460\tPrecision: 0.7454\tRecall: 0.7460\tF1_score: 0.7402\n",
      "medical_help:\n",
      "\tAccuracy: 0.9242\tPrecision: 0.9009\tRecall: 0.9242\tF1_score: 0.8998\n",
      "medical_products:\n",
      "\tAccuracy: 0.9519\tPrecision: 0.9403\tRecall: 0.9519\tF1_score: 0.9342\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.9740\tPrecision: 0.9648\tRecall: 0.9740\tF1_score: 0.9626\n",
      "security:\n",
      "\tAccuracy: 0.9813\tPrecision: 0.9695\tRecall: 0.9813\tF1_score: 0.9724\n",
      "military:\n",
      "\tAccuracy: 0.9651\tPrecision: 0.9532\tRecall: 0.9651\tF1_score: 0.9507\n",
      "water:\n",
      "\tAccuracy: 0.9524\tPrecision: 0.9464\tRecall: 0.9524\tF1_score: 0.9418\n",
      "food:\n",
      "\tAccuracy: 0.9370\tPrecision: 0.9320\tRecall: 0.9370\tF1_score: 0.9321\n",
      "shelter:\n",
      "\tAccuracy: 0.9399\tPrecision: 0.9347\tRecall: 0.9399\tF1_score: 0.9300\n",
      "clothing:\n",
      "\tAccuracy: 0.9872\tPrecision: 0.9846\tRecall: 0.9872\tF1_score: 0.9837\n",
      "money:\n",
      "\tAccuracy: 0.9782\tPrecision: 0.9751\tRecall: 0.9782\tF1_score: 0.9682\n",
      "missing_people:\n",
      "\tAccuracy: 0.9885\tPrecision: 0.9849\tRecall: 0.9885\tF1_score: 0.9832\n",
      "refugees:\n",
      "\tAccuracy: 0.9693\tPrecision: 0.9546\tRecall: 0.9693\tF1_score: 0.9571\n",
      "death:\n",
      "\tAccuracy: 0.9565\tPrecision: 0.9510\tRecall: 0.9565\tF1_score: 0.9407\n",
      "other_aid:\n",
      "\tAccuracy: 0.8688\tPrecision: 0.8279\tRecall: 0.8688\tF1_score: 0.8184\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.9336\tPrecision: 0.8882\tRecall: 0.9336\tF1_score: 0.9036\n",
      "transport:\n",
      "\tAccuracy: 0.9554\tPrecision: 0.9471\tRecall: 0.9554\tF1_score: 0.9360\n",
      "buildings:\n",
      "\tAccuracy: 0.9539\tPrecision: 0.9454\tRecall: 0.9539\tF1_score: 0.9378\n",
      "electricity:\n",
      "\tAccuracy: 0.9814\tPrecision: 0.9751\tRecall: 0.9814\tF1_score: 0.9733\n",
      "tools:\n",
      "\tAccuracy: 0.9928\tPrecision: 0.9859\tRecall: 0.9928\tF1_score: 0.9893\n",
      "hospitals:\n",
      "\tAccuracy: 0.9894\tPrecision: 0.9789\tRecall: 0.9894\tF1_score: 0.9841\n",
      "shops:\n",
      "\tAccuracy: 0.9955\tPrecision: 0.9911\tRecall: 0.9955\tF1_score: 0.9933\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9892\tPrecision: 0.9786\tRecall: 0.9892\tF1_score: 0.9839\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.9539\tPrecision: 0.9108\tRecall: 0.9539\tF1_score: 0.9318\n",
      "weather_related:\n",
      "\tAccuracy: 0.8625\tPrecision: 0.8603\tRecall: 0.8625\tF1_score: 0.8560\n",
      "floods:\n",
      "\tAccuracy: 0.9464\tPrecision: 0.9449\tRecall: 0.9464\tF1_score: 0.9363\n",
      "storm:\n",
      "\tAccuracy: 0.9305\tPrecision: 0.9244\tRecall: 0.9305\tF1_score: 0.9196\n",
      "fire:\n",
      "\tAccuracy: 0.9891\tPrecision: 0.9892\tRecall: 0.9891\tF1_score: 0.9840\n",
      "earthquake:\n",
      "\tAccuracy: 0.9670\tPrecision: 0.9656\tRecall: 0.9670\tF1_score: 0.9653\n",
      "cold:\n",
      "\tAccuracy: 0.9813\tPrecision: 0.9749\tRecall: 0.9813\tF1_score: 0.9737\n",
      "other_weather:\n",
      "\tAccuracy: 0.9436\tPrecision: 0.9141\tRecall: 0.9436\tF1_score: 0.9201\n",
      "direct_report:\n",
      "\tAccuracy: 0.8459\tPrecision: 0.8324\tRecall: 0.8459\tF1_score: 0.8190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Predicts messages in the test set based on categories in the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]))\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "    \n",
    "    parameters = {\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__estimator__n_estimators': [20,30]\n",
    "        }\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_d...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'features__text_pipeline__tfidf__use_idf': (True, False), 'clf__estimator__n_estimators': [20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = model_pipeline2()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.9973\tPrecision: 0.9973\tRecall: 0.9973\tF1_score: 0.9973\n",
      "request:\n",
      "\tAccuracy: 0.9971\tPrecision: 0.9971\tRecall: 0.9971\tF1_score: 0.9971\n",
      "offer:\n",
      "\tAccuracy: 0.9995\tPrecision: 0.9995\tRecall: 0.9995\tF1_score: 0.9995\n",
      "aid_related:\n",
      "\tAccuracy: 0.9978\tPrecision: 0.9978\tRecall: 0.9978\tF1_score: 0.9978\n",
      "medical_help:\n",
      "\tAccuracy: 0.9978\tPrecision: 0.9979\tRecall: 0.9978\tF1_score: 0.9978\n",
      "medical_products:\n",
      "\tAccuracy: 0.9978\tPrecision: 0.9979\tRecall: 0.9978\tF1_score: 0.9978\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.9979\tPrecision: 0.9979\tRecall: 0.9979\tF1_score: 0.9979\n",
      "security:\n",
      "\tAccuracy: 0.9984\tPrecision: 0.9984\tRecall: 0.9984\tF1_score: 0.9983\n",
      "military:\n",
      "\tAccuracy: 0.9994\tPrecision: 0.9994\tRecall: 0.9994\tF1_score: 0.9994\n",
      "water:\n",
      "\tAccuracy: 0.9993\tPrecision: 0.9993\tRecall: 0.9993\tF1_score: 0.9993\n",
      "food:\n",
      "\tAccuracy: 0.9993\tPrecision: 0.9993\tRecall: 0.9993\tF1_score: 0.9993\n",
      "shelter:\n",
      "\tAccuracy: 0.9987\tPrecision: 0.9987\tRecall: 0.9987\tF1_score: 0.9987\n",
      "clothing:\n",
      "\tAccuracy: 0.9994\tPrecision: 0.9994\tRecall: 0.9994\tF1_score: 0.9994\n",
      "money:\n",
      "\tAccuracy: 0.9986\tPrecision: 0.9986\tRecall: 0.9986\tF1_score: 0.9985\n",
      "missing_people:\n",
      "\tAccuracy: 0.9994\tPrecision: 0.9994\tRecall: 0.9994\tF1_score: 0.9994\n",
      "refugees:\n",
      "\tAccuracy: 0.9978\tPrecision: 0.9979\tRecall: 0.9978\tF1_score: 0.9978\n",
      "death:\n",
      "\tAccuracy: 0.9991\tPrecision: 0.9991\tRecall: 0.9991\tF1_score: 0.9991\n",
      "other_aid:\n",
      "\tAccuracy: 0.9943\tPrecision: 0.9943\tRecall: 0.9943\tF1_score: 0.9942\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.9963\tPrecision: 0.9963\tRecall: 0.9963\tF1_score: 0.9963\n",
      "transport:\n",
      "\tAccuracy: 0.9975\tPrecision: 0.9975\tRecall: 0.9975\tF1_score: 0.9975\n",
      "buildings:\n",
      "\tAccuracy: 0.9989\tPrecision: 0.9989\tRecall: 0.9989\tF1_score: 0.9989\n",
      "electricity:\n",
      "\tAccuracy: 0.9990\tPrecision: 0.9990\tRecall: 0.9990\tF1_score: 0.9990\n",
      "tools:\n",
      "\tAccuracy: 0.9995\tPrecision: 0.9995\tRecall: 0.9995\tF1_score: 0.9995\n",
      "hospitals:\n",
      "\tAccuracy: 0.9992\tPrecision: 0.9992\tRecall: 0.9992\tF1_score: 0.9992\n",
      "shops:\n",
      "\tAccuracy: 0.9996\tPrecision: 0.9996\tRecall: 0.9996\tF1_score: 0.9996\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9987\tPrecision: 0.9987\tRecall: 0.9987\tF1_score: 0.9987\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.9973\tPrecision: 0.9973\tRecall: 0.9973\tF1_score: 0.9972\n",
      "weather_related:\n",
      "\tAccuracy: 0.9976\tPrecision: 0.9976\tRecall: 0.9976\tF1_score: 0.9976\n",
      "floods:\n",
      "\tAccuracy: 0.9981\tPrecision: 0.9981\tRecall: 0.9981\tF1_score: 0.9980\n",
      "storm:\n",
      "\tAccuracy: 0.9991\tPrecision: 0.9991\tRecall: 0.9991\tF1_score: 0.9991\n",
      "fire:\n",
      "\tAccuracy: 0.9995\tPrecision: 0.9995\tRecall: 0.9995\tF1_score: 0.9995\n",
      "earthquake:\n",
      "\tAccuracy: 0.9984\tPrecision: 0.9984\tRecall: 0.9984\tF1_score: 0.9984\n",
      "cold:\n",
      "\tAccuracy: 0.9993\tPrecision: 0.9993\tRecall: 0.9993\tF1_score: 0.9993\n",
      "other_weather:\n",
      "\tAccuracy: 0.9969\tPrecision: 0.9969\tRecall: 0.9969\tF1_score: 0.9968\n",
      "direct_report:\n",
      "\tAccuracy: 0.9963\tPrecision: 0.9963\tRecall: 0.9963\tF1_score: 0.9962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model2.predict(X_train)\n",
    "\n",
    "multioutput_classification_report(y_train, y_pred)\n",
    "\n",
    "# Predicts messages in the test set based on categories in the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.8177\tPrecision: 0.8041\tRecall: 0.8177\tF1_score: 0.8008\n",
      "request:\n",
      "\tAccuracy: 0.8889\tPrecision: 0.8827\tRecall: 0.8889\tF1_score: 0.8759\n",
      "offer:\n",
      "\tAccuracy: 0.9960\tPrecision: 0.9920\tRecall: 0.9960\tF1_score: 0.9940\n",
      "aid_related:\n",
      "\tAccuracy: 0.7733\tPrecision: 0.7720\tRecall: 0.7733\tF1_score: 0.7709\n",
      "medical_help:\n",
      "\tAccuracy: 0.9258\tPrecision: 0.9059\tRecall: 0.9258\tF1_score: 0.9010\n",
      "medical_products:\n",
      "\tAccuracy: 0.9521\tPrecision: 0.9415\tRecall: 0.9521\tF1_score: 0.9340\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.9740\tPrecision: 0.9645\tRecall: 0.9740\tF1_score: 0.9629\n",
      "security:\n",
      "\tAccuracy: 0.9811\tPrecision: 0.9679\tRecall: 0.9811\tF1_score: 0.9723\n",
      "military:\n",
      "\tAccuracy: 0.9648\tPrecision: 0.9515\tRecall: 0.9648\tF1_score: 0.9518\n",
      "water:\n",
      "\tAccuracy: 0.9582\tPrecision: 0.9546\tRecall: 0.9582\tF1_score: 0.9506\n",
      "food:\n",
      "\tAccuracy: 0.9421\tPrecision: 0.9380\tRecall: 0.9421\tF1_score: 0.9378\n",
      "shelter:\n",
      "\tAccuracy: 0.9402\tPrecision: 0.9355\tRecall: 0.9402\tF1_score: 0.9300\n",
      "clothing:\n",
      "\tAccuracy: 0.9860\tPrecision: 0.9819\tRecall: 0.9860\tF1_score: 0.9812\n",
      "money:\n",
      "\tAccuracy: 0.9780\tPrecision: 0.9785\tRecall: 0.9780\tF1_score: 0.9676\n",
      "missing_people:\n",
      "\tAccuracy: 0.9885\tPrecision: 0.9886\tRecall: 0.9885\tF1_score: 0.9829\n",
      "refugees:\n",
      "\tAccuracy: 0.9706\tPrecision: 0.9644\tRecall: 0.9706\tF1_score: 0.9579\n",
      "death:\n",
      "\tAccuracy: 0.9590\tPrecision: 0.9555\tRecall: 0.9590\tF1_score: 0.9456\n",
      "other_aid:\n",
      "\tAccuracy: 0.8694\tPrecision: 0.8333\tRecall: 0.8694\tF1_score: 0.8165\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.9344\tPrecision: 0.8739\tRecall: 0.9344\tF1_score: 0.9031\n",
      "transport:\n",
      "\tAccuracy: 0.9560\tPrecision: 0.9470\tRecall: 0.9560\tF1_score: 0.9382\n",
      "buildings:\n",
      "\tAccuracy: 0.9562\tPrecision: 0.9518\tRecall: 0.9562\tF1_score: 0.9416\n",
      "electricity:\n",
      "\tAccuracy: 0.9816\tPrecision: 0.9783\tRecall: 0.9816\tF1_score: 0.9731\n",
      "tools:\n",
      "\tAccuracy: 0.9929\tPrecision: 0.9859\tRecall: 0.9929\tF1_score: 0.9894\n",
      "hospitals:\n",
      "\tAccuracy: 0.9892\tPrecision: 0.9789\tRecall: 0.9892\tF1_score: 0.9840\n",
      "shops:\n",
      "\tAccuracy: 0.9955\tPrecision: 0.9911\tRecall: 0.9955\tF1_score: 0.9933\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9891\tPrecision: 0.9786\tRecall: 0.9891\tF1_score: 0.9838\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.9539\tPrecision: 0.9108\tRecall: 0.9539\tF1_score: 0.9318\n",
      "weather_related:\n",
      "\tAccuracy: 0.8821\tPrecision: 0.8805\tRecall: 0.8821\tF1_score: 0.8781\n",
      "floods:\n",
      "\tAccuracy: 0.9462\tPrecision: 0.9450\tRecall: 0.9462\tF1_score: 0.9359\n",
      "storm:\n",
      "\tAccuracy: 0.9418\tPrecision: 0.9373\tRecall: 0.9418\tF1_score: 0.9363\n",
      "fire:\n",
      "\tAccuracy: 0.9888\tPrecision: 0.9834\tRecall: 0.9888\tF1_score: 0.9835\n",
      "earthquake:\n",
      "\tAccuracy: 0.9714\tPrecision: 0.9704\tRecall: 0.9714\tF1_score: 0.9704\n",
      "cold:\n",
      "\tAccuracy: 0.9814\tPrecision: 0.9756\tRecall: 0.9814\tF1_score: 0.9741\n",
      "other_weather:\n",
      "\tAccuracy: 0.9464\tPrecision: 0.9315\tRecall: 0.9464\tF1_score: 0.9252\n",
      "direct_report:\n",
      "\tAccuracy: 0.8548\tPrecision: 0.8444\tRecall: 0.8548\tF1_score: 0.8322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Builds a pipeline using the AdaBoost classifier\n",
    "pipeline_ada = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(\n",
    "        AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, class_weight='balanced'))\n",
    "    ))\n",
    "])\n",
    "\n",
    "parameters_ada = {\n",
    "    'clf__estimator__learning_rate': [0.1, 0.3],\n",
    "    'clf__estimator__n_estimators': [200, 400]\n",
    "}\n",
    "\n",
    "cv_ada = GridSearchCV(estimator=pipeline_ada, param_grid=parameters_ada, cv=3, scoring='f1_weighted', verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200, score=0.6208885536962466, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200, score=0.6178235428039128, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=200, score=0.6189139892051349, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400, score=0.6311540140277182, total= 9.5min\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400, score=0.6295619341926636, total= 9.5min\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__n_estimators=400, score=0.6285521138803205, total= 9.5min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200, score=0.6343908165005325, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200, score=0.6343592352109348, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=200, score=0.6334498098928178, total= 4.9min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400, score=0.6420730157214251, total= 9.5min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400, score=0.6361425794096324, total= 9.4min\n",
      "[CV] clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400 \n",
      "[CV]  clf__estimator__learning_rate=0.3, clf__estimator__n_estimators=400, score=0.6387288312853554, total= 9.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 92.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...er='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__estimator__learning_rate': [0.1, 0.3], 'clf__estimator__n_estimators': [200, 400]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produces a function that represents a line that best fits all the categories.\n",
    "# This line function can be used to estimate (predict) other results\n",
    "cv_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 0.3, 'clf__estimator__n_estimators': 400}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays parameters with the best results for the Adaboost model\n",
    "cv_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.7748\tPrecision: 0.8524\tRecall: 0.7748\tF1_score: 0.7909\n",
      "request:\n",
      "\tAccuracy: 0.8796\tPrecision: 0.8992\tRecall: 0.8796\tF1_score: 0.8860\n",
      "offer:\n",
      "\tAccuracy: 0.9908\tPrecision: 0.9969\tRecall: 0.9908\tF1_score: 0.9930\n",
      "aid_related:\n",
      "\tAccuracy: 0.7987\tPrecision: 0.7978\tRecall: 0.7987\tF1_score: 0.7979\n",
      "medical_help:\n",
      "\tAccuracy: 0.8998\tPrecision: 0.9370\tRecall: 0.8998\tF1_score: 0.9127\n",
      "medical_products:\n",
      "\tAccuracy: 0.9140\tPrecision: 0.9603\tRecall: 0.9140\tF1_score: 0.9303\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.9136\tPrecision: 0.9756\tRecall: 0.9136\tF1_score: 0.9372\n",
      "security:\n",
      "\tAccuracy: 0.9244\tPrecision: 0.9850\tRecall: 0.9244\tF1_score: 0.9484\n",
      "military:\n",
      "\tAccuracy: 0.9705\tPrecision: 0.9829\tRecall: 0.9705\tF1_score: 0.9745\n",
      "water:\n",
      "\tAccuracy: 0.9523\tPrecision: 0.9674\tRecall: 0.9523\tF1_score: 0.9571\n",
      "food:\n",
      "\tAccuracy: 0.9580\tPrecision: 0.9620\tRecall: 0.9580\tF1_score: 0.9594\n",
      "shelter:\n",
      "\tAccuracy: 0.9389\tPrecision: 0.9536\tRecall: 0.9389\tF1_score: 0.9437\n",
      "clothing:\n",
      "\tAccuracy: 0.9881\tPrecision: 0.9932\tRecall: 0.9881\tF1_score: 0.9897\n",
      "money:\n",
      "\tAccuracy: 0.9615\tPrecision: 0.9849\tRecall: 0.9615\tF1_score: 0.9697\n",
      "missing_people:\n",
      "\tAccuracy: 0.9712\tPrecision: 0.9918\tRecall: 0.9712\tF1_score: 0.9790\n",
      "refugees:\n",
      "\tAccuracy: 0.9350\tPrecision: 0.9727\tRecall: 0.9350\tF1_score: 0.9487\n",
      "death:\n",
      "\tAccuracy: 0.9595\tPrecision: 0.9735\tRecall: 0.9595\tF1_score: 0.9642\n",
      "other_aid:\n",
      "\tAccuracy: 0.7963\tPrecision: 0.8752\tRecall: 0.7963\tF1_score: 0.8220\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.8129\tPrecision: 0.9389\tRecall: 0.8129\tF1_score: 0.8562\n",
      "transport:\n",
      "\tAccuracy: 0.8991\tPrecision: 0.9593\tRecall: 0.8991\tF1_score: 0.9211\n",
      "buildings:\n",
      "\tAccuracy: 0.9337\tPrecision: 0.9643\tRecall: 0.9337\tF1_score: 0.9442\n",
      "electricity:\n",
      "\tAccuracy: 0.9748\tPrecision: 0.9879\tRecall: 0.9748\tF1_score: 0.9792\n",
      "tools:\n",
      "\tAccuracy: 0.9865\tPrecision: 0.9960\tRecall: 0.9865\tF1_score: 0.9901\n",
      "hospitals:\n",
      "\tAccuracy: 0.9807\tPrecision: 0.9930\tRecall: 0.9807\tF1_score: 0.9851\n",
      "shops:\n",
      "\tAccuracy: 0.9928\tPrecision: 0.9972\tRecall: 0.9928\tF1_score: 0.9943\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9575\tPrecision: 0.9905\tRecall: 0.9575\tF1_score: 0.9705\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.8442\tPrecision: 0.9600\tRecall: 0.8442\tF1_score: 0.8866\n",
      "weather_related:\n",
      "\tAccuracy: 0.8999\tPrecision: 0.8991\tRecall: 0.8999\tF1_score: 0.8994\n",
      "floods:\n",
      "\tAccuracy: 0.9514\tPrecision: 0.9565\tRecall: 0.9514\tF1_score: 0.9534\n",
      "storm:\n",
      "\tAccuracy: 0.9523\tPrecision: 0.9617\tRecall: 0.9523\tF1_score: 0.9553\n",
      "fire:\n",
      "\tAccuracy: 0.9878\tPrecision: 0.9943\tRecall: 0.9878\tF1_score: 0.9900\n",
      "earthquake:\n",
      "\tAccuracy: 0.9731\tPrecision: 0.9745\tRecall: 0.9731\tF1_score: 0.9736\n",
      "cold:\n",
      "\tAccuracy: 0.9809\tPrecision: 0.9895\tRecall: 0.9809\tF1_score: 0.9837\n",
      "other_weather:\n",
      "\tAccuracy: 0.8867\tPrecision: 0.9567\tRecall: 0.8867\tF1_score: 0.9114\n",
      "direct_report:\n",
      "\tAccuracy: 0.8261\tPrecision: 0.8589\tRecall: 0.8261\tF1_score: 0.8366\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv_ada.predict(X_train)\n",
    "\n",
    "multioutput_classification_report(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related:\n",
      "\tAccuracy: 0.7458\tPrecision: 0.8242\tRecall: 0.7458\tF1_score: 0.7634\n",
      "request:\n",
      "\tAccuracy: 0.8600\tPrecision: 0.8776\tRecall: 0.8600\tF1_score: 0.8665\n",
      "offer:\n",
      "\tAccuracy: 0.9811\tPrecision: 0.9925\tRecall: 0.9811\tF1_score: 0.9867\n",
      "aid_related:\n",
      "\tAccuracy: 0.7827\tPrecision: 0.7814\tRecall: 0.7827\tF1_score: 0.7814\n",
      "medical_help:\n",
      "\tAccuracy: 0.8663\tPrecision: 0.9145\tRecall: 0.8663\tF1_score: 0.8851\n",
      "medical_products:\n",
      "\tAccuracy: 0.8980\tPrecision: 0.9423\tRecall: 0.8980\tF1_score: 0.9158\n",
      "search_and_rescue:\n",
      "\tAccuracy: 0.8892\tPrecision: 0.9624\tRecall: 0.8892\tF1_score: 0.9208\n",
      "security:\n",
      "\tAccuracy: 0.8941\tPrecision: 0.9686\tRecall: 0.8941\tF1_score: 0.9280\n",
      "military:\n",
      "\tAccuracy: 0.9553\tPrecision: 0.9673\tRecall: 0.9553\tF1_score: 0.9601\n",
      "water:\n",
      "\tAccuracy: 0.9496\tPrecision: 0.9654\tRecall: 0.9496\tF1_score: 0.9549\n",
      "food:\n",
      "\tAccuracy: 0.9462\tPrecision: 0.9517\tRecall: 0.9462\tF1_score: 0.9482\n",
      "shelter:\n",
      "\tAccuracy: 0.9296\tPrecision: 0.9419\tRecall: 0.9296\tF1_score: 0.9342\n",
      "clothing:\n",
      "\tAccuracy: 0.9783\tPrecision: 0.9859\tRecall: 0.9783\tF1_score: 0.9814\n",
      "money:\n",
      "\tAccuracy: 0.9519\tPrecision: 0.9760\tRecall: 0.9519\tF1_score: 0.9617\n",
      "missing_people:\n",
      "\tAccuracy: 0.9559\tPrecision: 0.9819\tRecall: 0.9559\tF1_score: 0.9678\n",
      "refugees:\n",
      "\tAccuracy: 0.9172\tPrecision: 0.9618\tRecall: 0.9172\tF1_score: 0.9359\n",
      "death:\n",
      "\tAccuracy: 0.9484\tPrecision: 0.9603\tRecall: 0.9484\tF1_score: 0.9531\n",
      "other_aid:\n",
      "\tAccuracy: 0.7569\tPrecision: 0.8391\tRecall: 0.7569\tF1_score: 0.7871\n",
      "infrastructure_related:\n",
      "\tAccuracy: 0.7822\tPrecision: 0.9133\tRecall: 0.7822\tF1_score: 0.8325\n",
      "transport:\n",
      "\tAccuracy: 0.8621\tPrecision: 0.9338\tRecall: 0.8621\tF1_score: 0.8925\n",
      "buildings:\n",
      "\tAccuracy: 0.9221\tPrecision: 0.9532\tRecall: 0.9221\tF1_score: 0.9340\n",
      "electricity:\n",
      "\tAccuracy: 0.9623\tPrecision: 0.9817\tRecall: 0.9623\tF1_score: 0.9701\n",
      "tools:\n",
      "\tAccuracy: 0.9745\tPrecision: 0.9876\tRecall: 0.9745\tF1_score: 0.9808\n",
      "hospitals:\n",
      "\tAccuracy: 0.9677\tPrecision: 0.9845\tRecall: 0.9677\tF1_score: 0.9753\n",
      "shops:\n",
      "\tAccuracy: 0.9825\tPrecision: 0.9921\tRecall: 0.9825\tF1_score: 0.9871\n",
      "aid_centers:\n",
      "\tAccuracy: 0.9487\tPrecision: 0.9838\tRecall: 0.9487\tF1_score: 0.9647\n",
      "other_infrastructure:\n",
      "\tAccuracy: 0.8144\tPrecision: 0.9345\tRecall: 0.8144\tF1_score: 0.8634\n",
      "weather_related:\n",
      "\tAccuracy: 0.8909\tPrecision: 0.8900\tRecall: 0.8909\tF1_score: 0.8904\n",
      "floods:\n",
      "\tAccuracy: 0.9336\tPrecision: 0.9389\tRecall: 0.9336\tF1_score: 0.9359\n",
      "storm:\n",
      "\tAccuracy: 0.9379\tPrecision: 0.9465\tRecall: 0.9379\tF1_score: 0.9410\n",
      "fire:\n",
      "\tAccuracy: 0.9769\tPrecision: 0.9859\tRecall: 0.9769\tF1_score: 0.9809\n",
      "earthquake:\n",
      "\tAccuracy: 0.9676\tPrecision: 0.9691\tRecall: 0.9676\tF1_score: 0.9682\n",
      "cold:\n",
      "\tAccuracy: 0.9713\tPrecision: 0.9800\tRecall: 0.9713\tF1_score: 0.9749\n",
      "other_weather:\n",
      "\tAccuracy: 0.8535\tPrecision: 0.9270\tRecall: 0.8535\tF1_score: 0.8835\n",
      "direct_report:\n",
      "\tAccuracy: 0.8047\tPrecision: 0.8430\tRecall: 0.8047\tF1_score: 0.8174\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv_ada.predict(X_test)\n",
    "\n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adaboost_cv.pkl', 'wb') as file:\n",
    "    pickle.dump(cv_ada, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
